# Config for multi-node GRPO in dev/grpo_full_finetune_distributed.py
# using a Llama3.2 3B Base model
#
# This config assumes that you've run the following command before launching
# this run:
#   tune download meta-llama/Llama-3.2-3B --output-dir /fs/cml-projects/guardian_models/models/Llama-3.2-3B --ignore-patterns "original/consolidated.00.pth"
#
# It can be beneficial to first train the base model with SFT using the 3B_sft recipe.
#
# To launch on 4 devices, run the following command from root:
# tune run --nproc_per_node 4 dev/grpo_full_finetune_distributed --config dev/3B_full_grpo
#
# You can add specific overrides through the command line. For example
# to override the checkpointer directory while launching training
# you can run:
#   tune run --nproc_per_node 4 dev/grpo_full_finetune_distributed --config dev/grpo/3B_full_rl checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
#
# This config works best when the model is being fine-tuned on 2+ GPUs.
#
# Furthermore, you can launch it on multiple nodes by going to recipes/dev/ and using
#  sbatch multinode_grpo.sbatch

name: grpo_llama3b

output_dir: /fs/cml-projects/guardian_models/models/checkpoints/${name}
base_model_path: /fs/cml-projects/guardian_models/models/Qwen2-1.5B-Instruct # Use this to train from the slightly trained SFT model

# Tokenizer
tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  path: /fs/cml-projects/guardian_models/models/Qwen2-1.5B-Instruct/vocab.json
  merges_file: /fs/cml-projects/guardian_models/models/Qwen2-1.5B-Instruct/merges.txt
  max_seq_len: null

# Dataset
dataset:
  # _component_: torchtune.dev.grpo.gsm8k.gsm8k_dataset
  # partition: 1-9/10
# dataset:
  _component_: torchtune.dev.grpo.compliance.compliance_dataset
  source: json
  data_files: data/easy_train_8872.jsonl
  split: train
#   new_system_prompt: "
# You are a guardian model evaluating the compliance of a chatbot agent to various rules. 
# You will be given a rule that the agent must follow, along with a conversation between the agent and a user. 
# The conversation will be in the following format:\n'User': [user query]\n'Agent': [agent response]\n
# Your task is to evaluate the conversation in the context of the rule, and determine whether the rule has been violated by the agent. 
# Output your response within xml tags for both the answer and reasoning supporting that answer. 
# First provide the reasoning of how the conversation relates to the rule and how the chatbot either violated or did not violate the rule. 
# The rule may not be applicable to the conversation, and in that case it canot possibly be violated because it does not apply.
# Cite specific examples of how the rule was violated or not violated. If it was not violated, either cite evidence of the agent following the rule, or cite wording in the
# rule and conversation that show by definition of terms that the rule is not applicable to the specific conversation.
# Then, give the answer as either PASS for not violated or FAIL for violated. 

# Respond in the following format:
# <reasoning>
# [your reasoning]
# </reasoning>
# <answer>
# PASS/FAIL
# </answer>
# "
valid_data_files: data/easy_validation_1765.jsonl
seed: null
shuffle: True

# Model Arguments
model:
  _component_: torchtune.models.qwen2.qwen2_1_5b

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${base_model_path}
  checkpoint_files: [
    model.safetensors
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}
  model_type: QWEN2


ref_checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${base_model_path}
  checkpoint_files: [
    model.safetensors
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}/ref  # shouldn't be used?
  model_type: QWEN2


resume_from_checkpoint: False
save_every_n_epochs: 1

# Fine-tuning arguments
batch_size: 1
grpo_samples: 6
forward_batch_size: 1
max_generated_tokens: 512
gradient_accumulation_steps: 1
top_k: null
temperature: 1.0

ppo_epochs: 1

num_steps: 200

clip_grad_norm: 1.0

epochs: 10
optimizer:
  _component_: torch.optim.AdamW
  lr: 1e-5
  fused: True
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 50
loss:
  _component_: torchtune.dev.grpo.loss.GRPOSimpleLoss
  
  kl_coeff: 0.01
  epsilon: 0.2

# Training env
device: cuda

# Memory management
enable_activation_checkpointing: True  # True reduces memory
compile: False  # pytorch compile, set to true for better perf/memory

# Reduced precision
dtype: bf16


# Logging
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: ${output_dir}/logs
log_every_n_steps: 1
log_peak_memory_stats: True

# Profiler (disabled)
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: True

  #Output directory of trace artifacts
  output_dir: ${output_dir}/profiling_outputs

  #`torch.profiler.ProfilerActivity` types to trace
  cpu: True
  cuda: True

  #trace options passed to `torch.profiler.profile`
  profile_memory: True
  with_stack: True
  record_shapes: True
  with_flops: False

  # `torch.profiler.schedule` options:
  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
  wait_steps: 5
  warmup_steps: 3
  active_steps: 2
  num_cycles: 1
