{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e635b406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vulcanscratch/mhoover4/miniconda3/envs/unsloth/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "def get_balanced_subset(dataset, label_column=\"label\", target_count=-1, seed=42,):\n",
    "    labels = dataset[label_column]\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"Initial label counts: {label_counts}\")\n",
    "\n",
    "    # Determine the maximum number of samples per class\n",
    "    # Give yourself 2% margin\n",
    "    target_count = min(label_counts.values()) if target_count == -1 else min(min(label_counts.values()), target_count)\n",
    "    # target_count = int(np.ceil(1.02 * target_count))\n",
    "    \n",
    "    # Build balanced subsets for each class\n",
    "    subsets = []\n",
    "    for label_value in label_counts:\n",
    "        # Filter for the current class, shuffle, then select target_count samples\n",
    "        class_subset = (\n",
    "            dataset\n",
    "            .filter(lambda example, lv=label_value: example[label_column] == lv)\n",
    "            .shuffle(seed=seed)\n",
    "            .select(range(target_count))\n",
    "        )\n",
    "        subsets.append(class_subset)\n",
    "\n",
    "    # Concatenate and shuffle the balanced subsets\n",
    "    balanced = concatenate_datasets(subsets)\n",
    "    balanced = balanced.shuffle(seed=seed)\n",
    "\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee7ea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test_handcrafted split: 100%|██████████| 199/199 [00:00<00:00, 2023.81 examples/s]\n",
      "Generating train split: 100%|██████████| 17930/17930 [00:00<00:00, 81583.89 examples/s]\n",
      "Generating train_cot split: 100%|██████████| 17930/17930 [00:00<00:00, 55602.57 examples/s]\n",
      "Generating test split: 100%|██████████| 98/98 [00:00<00:00, 14373.60 examples/s]\n",
      "Generating train_orig_cot split: 100%|██████████| 6247/6247 [00:00<00:00, 52840.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d = datasets.load_dataset(\"tomg-group-umd/compliance\", \"compliance\", split=\"train_orig_cot\")\n",
    "print(len(d))\n",
    "# d.select(range(2000)).to_json(\"data/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial label counts: Counter({'PASS': 11693, 'FAIL': 3541})\n",
      "Initial label counts: Counter({'PASS': 1907, 'FAIL': 1751})\n",
      "Initial label counts: Counter({'FAIL': 16013, 'PASS': 11153})\n",
      "Initial label counts: Counter({'FAIL': 14552, 'PASS': 12614})\n",
      "Initial label counts: Counter({'FAIL': 1469, 'PASS': 1395})\n",
      "Initial label counts: Counter({'FAIL': 1455, 'PASS': 1409})\n",
      "Initial label counts: Counter({'PASS': 29027, 'FAIL': 7786})\n",
      "Initial label counts: Counter({'FAIL': 19015, 'PASS': 18934})\n",
      "Initial label counts: Counter({'PASS': 19101, 'FAIL': 18848})\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "6000\n",
      "11930\n",
      "Initial label counts: Counter({'PASS': 9875, 'FAIL': 8055})\n",
      "34110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 35/35 [00:00<00:00, 41.30ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90521522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Safetey datasets\n",
    "s1 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"aegis\", split=\"train_harm\")\n",
    "s2 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"aegis\", split=\"train_refusal_cot\")\n",
    "s3 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"beavertails\", split=\"train_harm_cot\")\n",
    "s4 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"beavertails\", split=\"train_refusal\")\n",
    "s5 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"toxicchat\", split=\"train_jailbreaking\")\n",
    "s6 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"toxicchat\", split=\"train_refusal_cot\")\n",
    "s7 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"wildguard\", split=\"train_harm_cot\")\n",
    "s8 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"wildguard\", split=\"train_jailbreaking\")\n",
    "s9 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"wildguard\", split=\"train_refusal\")\n",
    "\n",
    "s = [s1, s2, s3, s4, s5, s6, s7, s8, s9]\n",
    "for i in range(len(s)):\n",
    "    s[i] = get_balanced_subset(s[i]).shuffle(seed=42).select(range(2000))\n",
    "for i in range(len(s)):\n",
    "    print(len(s[i]))\n",
    "\n",
    "# Compliance datasets\n",
    "d1 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"compliance\", split=\"train_cot\")\n",
    "d2 = datasets.load_dataset(\"tomg-group-umd/compliance\", \"compliance\", split=\"train\")\n",
    "d1 = d1.select(range(6000)).shuffle(seed=42)\n",
    "d2 = d2.select(range(6000, 17930)).shuffle(seed=42)\n",
    "print(len(d1))\n",
    "print(len(d2))\n",
    "t1 = get_balanced_subset(datasets.concatenate_datasets([d1.select(range(185)), d2.select(range(315))]).shuffle(seed=42)).to_json(\"data/train_500.jsonl\")     \n",
    "t2 = get_balanced_subset(datasets.concatenate_datasets([d1.select(range(375)), d2.select(range(625))]).shuffle(seed=42)).to_json(\"data/train_1000.jsonl\")    \n",
    "t3 = get_balanced_subset(datasets.concatenate_datasets([d1.select(range(750)), d2.select(range(1250))]).shuffle(seed=42)).to_json(\"data/train_2000.jsonl\")   \n",
    "t4 = get_balanced_subset(datasets.concatenate_datasets([d1.select(range(1500)), d2.select(range(2500))]).shuffle(seed=42)).to_json(\"data/train_4000.jsonl\")  \n",
    "t5 = get_balanced_subset(datasets.concatenate_datasets([d1.select(range(3000)), d2.select(range(5000))]).shuffle(seed=42)).to_json(\"data/train_8000.jsonl\")  \n",
    "t6 = get_balanced_subset(datasets.concatenate_datasets([d1.select(range(6000)), d2.select(range(10000))]).shuffle(seed=42)).to_json(\"data/train_16000.jsonl\")  \n",
    "\n",
    "t = get_balanced_subset(datasets.concatenate_datasets([d1, d2]).shuffle(seed=42))\n",
    "\n",
    "# Mix dataset\n",
    "final = datasets.concatenate_datasets(s + [t]).shuffle(seed=42)\n",
    "print(len(final))\n",
    "final.to_json(\"data/train_32000_mix.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0fade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
